use crate::errors::{DriftError, Result};
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::time::{Duration, Instant};
use parking_lot::{RwLock, Mutex};
use tokio::sync::oneshot;
use uuid::Uuid;

/// Query cancellation and timeout management
pub struct QueryCancellationManager {
    active_queries: Arc<RwLock<HashMap<Uuid, QueryHandle>>>,
    config: CancellationConfig,
    stats: Arc<RwLock<CancellationStats>>,
    resource_monitor: Arc<ResourceMonitor>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CancellationConfig {
    pub default_timeout: Duration,
    pub max_timeout: Duration,
    pub enable_deadlock_detection: bool,
    pub deadlock_check_interval: Duration,
    pub memory_limit_mb: usize,
    pub cpu_limit_percent: f64,
    pub max_concurrent_queries: usize,
    pub kill_on_timeout: bool,
    pub graceful_shutdown_timeout: Duration,
}

impl Default for CancellationConfig {
    fn default() -> Self {
        Self {
            default_timeout: Duration::from_secs(300), // 5 minutes
            max_timeout: Duration::from_secs(3600), // 1 hour
            enable_deadlock_detection: true,
            deadlock_check_interval: Duration::from_secs(10),
            memory_limit_mb: 1024, // 1GB per query
            cpu_limit_percent: 80.0,
            max_concurrent_queries: 100,
            kill_on_timeout: true,
            graceful_shutdown_timeout: Duration::from_secs(30),
        }
    }
}

/// Handle for an active query that can be cancelled
pub struct QueryHandle {
    pub id: Uuid,
    pub query: String,
    pub started_at: Instant,
    pub timeout: Duration,
    pub state: Arc<RwLock<QueryState>>,
    pub cancel_flag: Arc<AtomicBool>,
    pub cancel_sender: Option<oneshot::Sender<()>>,
    pub progress: Arc<AtomicU64>,
    pub resource_usage: Arc<RwLock<ResourceUsage>>,
    pub metadata: HashMap<String, String>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum QueryState {
    Running,
    Cancelling,
    Cancelled,
    Completed,
    Failed,
    TimedOut,
    ResourceExceeded,
}

#[derive(Debug, Default, Clone, Serialize, Deserialize)]
pub struct ResourceUsage {
    pub memory_bytes: u64,
    pub cpu_time_ms: u64,
    pub rows_processed: u64,
    pub bytes_scanned: u64,
    pub temp_space_bytes: u64,
    pub network_bytes: u64,
}

#[derive(Debug, Default)]
pub struct CancellationStats {
    pub queries_started: u64,
    pub queries_completed: u64,
    pub queries_cancelled: u64,
    pub queries_timed_out: u64,
    pub queries_resource_exceeded: u64,
    pub queries_failed: u64,
    pub total_query_time_ms: u64,
    pub avg_query_time_ms: u64,
    pub max_query_time_ms: u64,
    pub active_query_count: usize,
}

/// Monitor system resources
struct ResourceMonitor {
    memory_limit: usize,
    cpu_limit: f64,
    current_memory: Arc<AtomicU64>,
    current_cpu: Arc<Mutex<f64>>,
}

/// Token for query execution that checks cancellation
pub struct CancellationToken {
    query_id: Uuid,
    cancel_flag: Arc<AtomicBool>,
    cancel_receiver: Option<oneshot::Receiver<()>>,
    progress: Arc<AtomicU64>,
    check_interval: Duration,
    last_check: Instant,
}

impl QueryCancellationManager {
    pub fn new(config: CancellationConfig) -> Self {
        let resource_monitor = Arc::new(ResourceMonitor::new(
            config.memory_limit_mb,
            config.cpu_limit_percent,
        ));

        let manager = Self {
            active_queries: Arc::new(RwLock::new(HashMap::new())),
            config: config.clone(),
            stats: Arc::new(RwLock::new(CancellationStats::default())),
            resource_monitor,
        };

        // Start background monitoring tasks
        if config.enable_deadlock_detection {
            manager.start_deadlock_detection();
        }

        manager.start_timeout_monitoring();
        manager.start_resource_monitoring();

        manager
    }

    /// Register a new query for execution
    pub fn register_query(
        &self,
        query: String,
        timeout: Option<Duration>,
        metadata: HashMap<String, String>,
    ) -> Result<CancellationToken> {
        // Check concurrent query limit
        let active_count = self.active_queries.read().len();
        if active_count >= self.config.max_concurrent_queries {
            return Err(DriftError::Other(format!(
                "Maximum concurrent queries ({}) exceeded",
                self.config.max_concurrent_queries
            )));
        }

        let query_id = Uuid::new_v4();
        let timeout = timeout
            .unwrap_or(self.config.default_timeout)
            .min(self.config.max_timeout);

        let (cancel_sender, cancel_receiver) = oneshot::channel();
        let cancel_flag = Arc::new(AtomicBool::new(false));

        let handle = QueryHandle {
            id: query_id,
            query: query.clone(),
            started_at: Instant::now(),
            timeout,
            state: Arc::new(RwLock::new(QueryState::Running)),
            cancel_flag: cancel_flag.clone(),
            cancel_sender: Some(cancel_sender),
            progress: Arc::new(AtomicU64::new(0)),
            resource_usage: Arc::new(RwLock::new(ResourceUsage::default())),
            metadata,
        };

        self.active_queries.write().insert(query_id, handle);

        // Update stats
        self.stats.write().queries_started += 1;
        self.stats.write().active_query_count += 1;

        Ok(CancellationToken {
            query_id,
            cancel_flag,
            cancel_receiver: Some(cancel_receiver),
            progress: Arc::new(AtomicU64::new(0)),
            check_interval: Duration::from_millis(100),
            last_check: Instant::now(),
        })
    }

    /// Cancel a specific query
    pub fn cancel_query(&self, query_id: Uuid) -> Result<()> {
        let mut queries = self.active_queries.write();

        if let Some(mut handle) = queries.get_mut(&query_id) {
            let mut state = handle.state.write();

            match *state {
                QueryState::Running => {
                    *state = QueryState::Cancelling;
                    handle.cancel_flag.store(true, Ordering::SeqCst);

                    // Send cancellation signal
                    if let Some(sender) = handle.cancel_sender.take() {
                        let _ = sender.send(());
                    }

                    Ok(())
                }
                QueryState::Cancelling => {
                    Ok(()) // Already cancelling
                }
                _ => {
                    Err(DriftError::Other(format!(
                        "Query {} is not running (state: {:?})",
                        query_id, *state
                    )))
                }
            }
        } else {
            Err(DriftError::NotFound(format!("Query {} not found", query_id)))
        }
    }

    /// Cancel all queries matching criteria
    pub fn cancel_queries_matching<F>(&self, predicate: F) -> Vec<Uuid>
    where
        F: Fn(&QueryHandle) -> bool,
    {
        let queries = self.active_queries.read();
        let mut cancelled = Vec::new();

        for (id, handle) in queries.iter() {
            if predicate(handle) {
                if self.cancel_query(*id).is_ok() {
                    cancelled.push(*id);
                }
            }
        }

        cancelled
    }

    /// Mark query as completed
    pub fn complete_query(&self, query_id: Uuid, success: bool) -> Result<()> {
        let mut queries = self.active_queries.write();

        if let Some(handle) = queries.remove(&query_id) {
            let new_state = if success {
                QueryState::Completed
            } else {
                QueryState::Failed
            };

            *handle.state.write() = new_state;

            // Update stats
            let elapsed = handle.started_at.elapsed();
            let elapsed_ms = elapsed.as_millis() as u64;

            let mut stats = self.stats.write();
            stats.active_query_count = stats.active_query_count.saturating_sub(1);

            if success {
                stats.queries_completed += 1;
            } else {
                stats.queries_failed += 1;
            }

            stats.total_query_time_ms += elapsed_ms;
            stats.max_query_time_ms = stats.max_query_time_ms.max(elapsed_ms);

            if stats.queries_completed > 0 {
                stats.avg_query_time_ms = stats.total_query_time_ms / stats.queries_completed;
            }

            Ok(())
        } else {
            Err(DriftError::NotFound(format!("Query {} not found", query_id)))
        }
    }

    /// Get status of a query
    pub fn get_query_status(&self, query_id: Uuid) -> Option<QueryStatus> {
        let queries = self.active_queries.read();

        queries.get(&query_id).map(|handle| {
            let state = *handle.state.read();
            let elapsed = handle.started_at.elapsed();
            let progress = handle.progress.load(Ordering::Relaxed);
            let resource_usage = handle.resource_usage.read().clone();

            QueryStatus {
                id: query_id,
                query: handle.query.clone(),
                state,
                elapsed,
                timeout: handle.timeout,
                progress_percent: progress as f64,
                resource_usage,
                metadata: handle.metadata.clone(),
            }
        })
    }

    /// List all active queries
    pub fn list_active_queries(&self) -> Vec<QueryStatus> {
        let queries = self.active_queries.read();

        queries.keys()
            .filter_map(|id| self.get_query_status(*id))
            .collect()
    }

    /// Start monitoring for query timeouts
    fn start_timeout_monitoring(&self) {
        let queries = self.active_queries.clone();
        let config = self.config.clone();
        let stats = self.stats.clone();

        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(1));

            loop {
                interval.tick().await;

                let now = Instant::now();
                let mut timed_out = Vec::new();

                {
                    let queries_read = queries.read();
                    for (id, handle) in queries_read.iter() {
                        if handle.started_at.elapsed() > handle.timeout {
                            let state = *handle.state.read();
                            if state == QueryState::Running {
                                timed_out.push(*id);
                            }
                        }
                    }
                }

                // Handle timed out queries
                for id in timed_out {
                    let mut queries_write = queries.write();
                    if let Some(mut handle) = queries_write.get_mut(&id) {
                        *handle.state.write() = QueryState::TimedOut;
                        handle.cancel_flag.store(true, Ordering::SeqCst);

                        if let Some(sender) = handle.cancel_sender.take() {
                            let _ = sender.send(());
                        }

                        stats.write().queries_timed_out += 1;

                        if config.kill_on_timeout {
                            queries_write.remove(&id);
                            stats.write().active_query_count =
                                stats.write().active_query_count.saturating_sub(1);
                        }
                    }
                }
            }
        });
    }

    /// Start monitoring resource usage
    fn start_resource_monitoring(&self) {
        let queries = self.active_queries.clone();
        let config = self.config.clone();
        let stats = self.stats.clone();
        let monitor = self.resource_monitor.clone();

        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(5));

            loop {
                interval.tick().await;

                let mut exceeded = Vec::new();

                {
                    let queries_read = queries.read();
                    for (id, handle) in queries_read.iter() {
                        let usage = handle.resource_usage.read();

                        // Check memory limit
                        if usage.memory_bytes > (config.memory_limit_mb as u64 * 1024 * 1024) {
                            exceeded.push((*id, "memory"));
                        }

                        // Check CPU limit (simplified)
                        let cpu_percent = (usage.cpu_time_ms as f64) /
                                        (handle.started_at.elapsed().as_millis() as f64) * 100.0;

                        if cpu_percent > config.cpu_limit_percent {
                            exceeded.push((*id, "cpu"));
                        }
                    }
                }

                // Handle resource-exceeded queries
                for (id, resource) in exceeded {
                    let mut queries_write = queries.write();
                    if let Some(mut handle) = queries_write.get_mut(&id) {
                        *handle.state.write() = QueryState::ResourceExceeded;
                        handle.cancel_flag.store(true, Ordering::SeqCst);

                        if let Some(sender) = handle.cancel_sender.take() {
                            let _ = sender.send(());
                        }

                        stats.write().queries_resource_exceeded += 1;

                        tracing::warn!(
                            "Query {} exceeded {} limit and was cancelled",
                            id, resource
                        );
                    }
                }
            }
        });
    }

    /// Start deadlock detection
    fn start_deadlock_detection(&self) {
        let queries = self.active_queries.clone();
        let config = self.config.clone();

        tokio::spawn(async move {
            let mut interval = tokio::time::interval(config.deadlock_check_interval);

            loop {
                interval.tick().await;

                // Simplified deadlock detection
                // In a real implementation, would check for circular wait chains
                let queries_read = queries.read();

                // Check for queries that have been in "Cancelling" state for too long
                let mut stuck_queries = Vec::new();

                for (id, handle) in queries_read.iter() {
                    let state = *handle.state.read();
                    if state == QueryState::Cancelling {
                        if handle.started_at.elapsed() > Duration::from_secs(60) {
                            stuck_queries.push(*id);
                        }
                    }
                }

                drop(queries_read);

                // Force-kill stuck queries
                for id in stuck_queries {
                    let mut queries_write = queries.write();
                    if let Some(handle) = queries_write.remove(&id) {
                        tracing::error!(
                            "Force-killing stuck query {}: {}",
                            id, handle.query
                        );
                    }
                }
            }
        });
    }

    /// Shutdown all queries gracefully
    pub async fn shutdown(&self) -> Result<()> {
        let timeout = self.config.graceful_shutdown_timeout;
        let start = Instant::now();

        // Cancel all running queries
        let query_ids: Vec<Uuid> = {
            let queries = self.active_queries.read();
            queries.keys().cloned().collect()
        };

        for id in query_ids {
            let _ = self.cancel_query(id);
        }

        // Wait for queries to complete or timeout
        while !self.active_queries.read().is_empty() && start.elapsed() < timeout {
            tokio::time::sleep(Duration::from_millis(100)).await;
        }

        // Force-kill remaining queries
        let remaining = self.active_queries.write().len();
        if remaining > 0 {
            tracing::warn!("Force-killing {} remaining queries", remaining);
            self.active_queries.write().clear();
        }

        Ok(())
    }

    /// Get statistics
    pub fn stats(&self) -> CancellationStats {
        self.stats.read().clone()
    }

    /// Update resource usage for a query
    pub fn update_resource_usage(&self, query_id: Uuid, usage: ResourceUsage) -> Result<()> {
        let queries = self.active_queries.read();

        if let Some(handle) = queries.get(&query_id) {
            *handle.resource_usage.write() = usage;
            Ok(())
        } else {
            Err(DriftError::NotFound(format!("Query {} not found", query_id)))
        }
    }
}

impl CancellationToken {
    /// Check if query has been cancelled
    pub fn is_cancelled(&self) -> bool {
        self.cancel_flag.load(Ordering::Relaxed)
    }

    /// Check cancellation with throttling
    pub fn check_cancellation(&mut self) -> Result<()> {
        // Throttle checks to avoid overhead
        if self.last_check.elapsed() < self.check_interval {
            return Ok(());
        }

        self.last_check = Instant::now();

        if self.is_cancelled() {
            Err(DriftError::Other("Query cancelled".to_string()))
        } else {
            Ok(())
        }
    }

    /// Async wait for cancellation
    pub async fn wait_for_cancellation(&mut self) {
        if let Some(receiver) = self.cancel_receiver.take() {
            let _ = receiver.await;
        }
    }

    /// Update progress
    pub fn update_progress(&self, progress: u64) {
        self.progress.store(progress, Ordering::Relaxed);
    }

    /// Create a child token for nested operations
    pub fn child_token(&self) -> CancellationToken {
        CancellationToken {
            query_id: self.query_id,
            cancel_flag: self.cancel_flag.clone(),
            cancel_receiver: None, // Child doesn't own receiver
            progress: self.progress.clone(),
            check_interval: self.check_interval,
            last_check: Instant::now(),
        }
    }
}

impl ResourceMonitor {
    fn new(memory_limit_mb: usize, cpu_limit_percent: f64) -> Self {
        Self {
            memory_limit: memory_limit_mb,
            cpu_limit: cpu_limit_percent,
            current_memory: Arc::new(AtomicU64::new(0)),
            current_cpu: Arc::new(Mutex::new(0.0)),
        }
    }

    fn check_memory(&self, bytes: u64) -> bool {
        bytes <= (self.memory_limit as u64 * 1024 * 1024)
    }

    fn check_cpu(&self, percent: f64) -> bool {
        percent <= self.cpu_limit
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryStatus {
    pub id: Uuid,
    pub query: String,
    pub state: QueryState,
    pub elapsed: Duration,
    pub timeout: Duration,
    pub progress_percent: f64,
    pub resource_usage: ResourceUsage,
    pub metadata: HashMap<String, String>,
}

impl Clone for CancellationStats {
    fn clone(&self) -> Self {
        Self {
            queries_started: self.queries_started,
            queries_completed: self.queries_completed,
            queries_cancelled: self.queries_cancelled,
            queries_timed_out: self.queries_timed_out,
            queries_resource_exceeded: self.queries_resource_exceeded,
            queries_failed: self.queries_failed,
            total_query_time_ms: self.total_query_time_ms,
            avg_query_time_ms: self.avg_query_time_ms,
            max_query_time_ms: self.max_query_time_ms,
            active_query_count: self.active_query_count,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_query_cancellation() {
        let manager = QueryCancellationManager::new(CancellationConfig::default());

        let mut token = manager.register_query(
            "SELECT * FROM large_table".to_string(),
            Some(Duration::from_secs(10)),
            HashMap::new(),
        ).unwrap();

        assert!(!token.is_cancelled());

        manager.cancel_query(token.query_id).unwrap();

        tokio::time::sleep(Duration::from_millis(10)).await;

        assert!(token.is_cancelled());
    }

    #[tokio::test]
    async fn test_query_timeout() {
        let mut config = CancellationConfig::default();
        config.default_timeout = Duration::from_millis(100);

        let manager = QueryCancellationManager::new(config);

        let mut token = manager.register_query(
            "SELECT * FROM large_table".to_string(),
            None,
            HashMap::new(),
        ).unwrap();

        tokio::time::sleep(Duration::from_millis(200)).await;

        // Query should be timed out
        let status = manager.get_query_status(token.query_id);
        assert!(status.is_none() || status.unwrap().state == QueryState::TimedOut);
    }

    #[test]
    fn test_concurrent_query_limit() {
        let mut config = CancellationConfig::default();
        config.max_concurrent_queries = 2;

        let manager = QueryCancellationManager::new(config);

        // Register two queries - should succeed
        let _token1 = manager.register_query(
            "SELECT 1".to_string(),
            None,
            HashMap::new(),
        ).unwrap();

        let _token2 = manager.register_query(
            "SELECT 2".to_string(),
            None,
            HashMap::new(),
        ).unwrap();

        // Third query should fail
        let result = manager.register_query(
            "SELECT 3".to_string(),
            None,
            HashMap::new(),
        );

        assert!(result.is_err());
    }
}